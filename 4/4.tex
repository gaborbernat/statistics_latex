\chapter*{Confidence interval}

\addcontentsline{toc}{chapter}{Confidence interval}
\setcounter{section}{0}
\renewcommand*{\theHsection}{ch4.\the\value{section}}

We observe the real world in order to understand the theoretical world; for this
we'll use the scientific and statistical models divised in the theoretical world
and use data from from the real world to estimate the paramters from the model.
We do this in hope that what conclusions we can make from our models will also
hold in the real world.


Now let us imagine the experiment of tossing a fair coin ten times. This
experiment has a binomial distribution with probability $\frac{1}{2}$, however
when amassing data from the real world we will not always get this proportion,
due to the sampling having its own distribution: for example extreme events
(like getting ten heads) are very unlikely, however getting half or close to
half of them heads is likely to happen. The question arrises where do we draw
the line, what are the values well likely to get most of the time?

Sometimes we will not have a model for our events: like in the case of flipping
a beer cap. If we were to perform a single experiment and get $m$ one side
out of $n$ events we can ask: was this a likely outcome? $m$ may be a sample of
any sample distribution (with its own parameters). For instance for the beer cap
let $n=1000$ and $m=576$. Our statistical model is binomial with a $1000$
samples, however we have no idea what's the probability of the model. 

An estimation is $\hat{p}=\frac{n}{m}=\frac{576}{1000}=0.576$. This may be a
good estimate, however it may be some other number, and so we ask what elso
could $p$ be? That is what is an interval that based on our existing experiments
the probability of getting one side of the beer cap could be? We refer to this
as the \emph{confidence interval}. 

The following methods are suppose that our data was taken in a form of simple
random sampling, and that our variables are independent; something that
statistical inference requires, otherwise we may need more complicated model.

So in conclusionwe can say that the goal of statistical inference is to draw
conclusions about a population parameter based on the data in a sample (and
statistics calculated from the data). A goal of statistical inference is to
identify a range of plausible values for a population parameter. A goal of
statistical inference is to identify a range of plausible values for a
population parameter. Inferential procedures can be used on data that are
collected from a random sample from a population.


\section{Confidence intervals with proportions}

Let us follow the example of the bottle cap. We are searching for the real $p$
with the given estiamte $\hat{p}$. The expected value of $\hat{p}$ is
$\mbox{E}(\hat{p})=p$ the variance is $\mbox{Var}(\hat{p}) =
\frac{p(1-p)}{n}=\frac{p(1-p)}{1000}$. Now according to the center limit theorem
because $p$ is the addition of lots and lots of small flips, it approximately
follows the normal distribution; that is $\hat{p} \approx
\mbox{Normal}\left(p, \frac{p(1-p)}{1000}\right)$. Now by performing a
reorganization: 

\[ \frac{\hat{p}-p}{\sqrt{p\cdot\frac{1-p}{n}}} \approx \mbox{Normal}(0,1) \]

Now for a normal distribution (and according to the empirical rule) the area
between $[-1.96, 1.96]$ covers $95\%$ of the area, so most likely our sample is
from this interval. In mathematical formula:


\[ \mbox{P} \left( \left| \frac{\hat{p}-p}{\sqrt{p\cdot\frac{1-p}{n}}} \right| >
1.96\right) = 0.05 = 5\%
\]

By writing up the reverse, and expanding we can conclude that, with
$z_{\frac{\alpha}{2}} = 1.96$:

\[ \mbox{P} \left( \underbrace{\hat{p} - \overbrace{z_{\frac{\alpha}{2}}\sqrt{p
\cdot \frac{1-p}{n}}}^{\mbox{margin of error}}}_{\mbox{lower limit}} \leq p \leq
\underbrace{\hat{p} + z_{\frac{\alpha}{2}}\sqrt{p \cdot
\frac{1-p}{n}}}_{\mbox{upper limit}} \right) = 95\%
\]
This means that we are $95\%$ confident that the value of $p$ is between is
lower and upper limit. Now the true value of $p$ is not random, however
$\hat{p}$ is as we took a random sample. Now the problem with this formula is
that while we do know $\hat{p},~p$ is unknown. One solution is to make
$\hat{p}=p$; or to make $p=\frac{1}{2}$ because that's the worst case, the
widest interval we can get.

For a given confidence interval $[a,b]$ the margin of error may be calculated
as:

\begin{align*}
a &= \hat{p} - \mbox{margin of error} \\
  b &= \hat{p} - \mbox{margin of error} \\ 
  \mbox{margin of error} &= \frac{b-a}{2}
\end{align*} 

Now modifying the area under the normal distribution that we take we can get
different confidence intervals for different probabilities. Now if you specify a
bigger confidence value, like $99\%$ you'll get a widder confidence interval.
It's up to you the trade off you are willing to accept.

Now assume you want to achive an $\alpha$ probability that you're wrong. In
this instance taken the graph of the normal distribution you want to find
$z_{\frac{\alpha}{2}}$ (y axis) such that the area remaining at each and is only
$\frac{\alpha}{2}$. In this case the area between intervals
$-z_{\frac{\alpha}{2}}$ to $z_{\frac{\alpha}{2}}$ is $1-\alpha$, which we are
looking over, as now we're missing $\alpha$ of the full area.

\section{Sample size for estimating a proportion}

Now in order to get a proportion the size of the sample has a major impact.
Let's see how we can determinate the sample size we need to find a given
proportion. We do not want to go overboard with this as we may just waste
resources or induce unrequired effort to collect it. We assume that data was
collected with simple random sampling from a population.

So a margin of error is: 

\[ \mbox{margin of error} = z_{\frac{\alpha}{2}}\sqrt{p \cdot
\frac{1-p}{n}}\]

For isntance, if our confidence interval is $95\%$, then 
$\frac{\alpha}{2}=\frac{1-0.95}{2}=0.025$, and we want a margin of error of
$\beta=0.03$. The question is what $n$ should be? For $95\%$ our normal quantile
is $1.96$. So:

\[ 0.03 = 1.96\sqrt{p \cdot
\frac{1-p}{n}}\]
 
 But we do not know what $p$ will be. To bypass this we plan for the worst case
 scenario, the expression $p\cdot(1-p)$ has its maximum at $p=\frac{1}{2}$,
 which also give our maximal margin of error for a given $n$. Now we resolve the
 equation:
 
 \[ n=\left( \frac{1.96 \cdot \frac{1}{2}}{0.03} \right)^2 = 1067 \]

Confidence intervals are about our confidence in the procedure to give us
correct results -- $95\%$ confidence intervals should contain the population
parameter $p 95\%$ of the time. If simple random samples are repeatedly taken
from the population, due to the randomness of the sampling process the observed
proportion of $\hat{p}$ will change from sample to sample and so will the
confidence interval constructed around $\hat{p}$. Of these confidence intervals,
$95\%$ will include the true population proportion $p$.

Note that $p$ is a population parameter and is therefore not a random variable.
For any interval, $p$ is either contained in the interval or not. Different
margin of error will result in different number of events required, however the
sample size increases quadratically as the margin of error decreases linearly.

\section{Confidence intervals for means}

In this case the data is not a categorical one, is instead a continous variable.
In this case the expected value is $\mu = \bar{X}$, and this is also our
estimation. The variance is equal to $\frac{\sigma^2}{n}$. Again if the data is
made up of the sum of bunch of small measurements, we can assume according
to the center limit theorem that $\bar{X} \approx \mbox{Normal}\left(\mu,
\frac{\sigma^2}{n}\right).$ Again we reduce this to the standard normal
distribution to get:

\[ \frac{\bar{X}-\mu}{\sqrt{\frac{\sigma^2}{n}}}  \approx \mbox{Normal}
\left( 0, 1\right)
\]

Now the problem now is that we do not know the value of $\sigma$. What would be
a good estimation for this? One solution is to use the standard deviation of $X$
(calculated with dividing with $n-1$), noted as $s$. However, while
$\mbox{E}(s^2)=\sigma^2$, substituting this into upper formula does not gives a
normal distribution; instead we'll have a $t$ distribution with $n-1$ degrees of
freedom:


\[ \frac{\bar{X}-\mu}{\sqrt{\frac{s^2}{n}}}  \approx t_{n-1}
\]

The $t$ distribution is similar to the normal one, however not quite there.
Increasing the degree of freedom reduces the difference between these two.
With this the $z_{\frac{\alpha}{2}}$ changes also, so you'll need to use a table
to get the correct number for a given number of freedom (which is $n-1$, where $n$ is
the sample size). The marginal error may be calculated as:

\[ \mbox{marginal error}= z_{\frac{\alpha}{2}} \cdot \sqrt{\frac{s^2}{n}} \]

We can use this to calculate the true mean from a sample mean. That is with a
given confidence we can say that our true mean is somewhere inside the
calculated confidence interval, which depends from the sample mean with the
calculated marginal error.

\section{Robustness for confidence intervals}

To use these methods many conditions must be satisfied, and an important part of
the statistical inference is to determine if these hold. Here are what we need
to make sure that they are true:

\begin{enumerate}
  \item the $n$ observations are independent,
  \item $n$ needs to be large enough, so that the central limit theorem may kick
  in and $\hat{p}$ to have a normal distribution, 
\end{enumerate}

For extreme theoretical $p$ values larger counts of samples are required to
achieve the same confidence interval. For instance in case of a coin flip if $p$
is $\frac{1}{2}$ a hundred samples may be enough for the central limit theorem
to kick in and achive a $95\%$ confidence. However, if $p=0.01$ we may need
$1000$ samples for the same confidence. An explanation for this is that the
normal distribution is a continous model, and we are using it to estimate 
discrete values.

In order for the confidence interval procedure for the true proportion to
provide reliable results, the total number of subjects surveyed should be large.
If the true population proportion is close to $0.5$ a sample of $100$ may be
adequate. However, if the true population proportion is closer to $0$ or $1$ a
larger sample is required.

Increasing the sample size will not eliminate non-response bias. In the presence
of non-response bias, the confidence interval may not cover the true population
parameter at the specified level of confidence, regardless of the sample size.
An assumption for the construction of confidence intervals is that respondents
are selected independently.

In case of means the conditions are:

\begin{enumerate}
  \item the $n$ observations are independent
  \item $n$ needs to be large enough so that $\bar{X}$ is approximatly normally
  distributed.
\end{enumerate}

The $t$ distribution works extremly well with even a low number of samples ($n
=10$) if the theoretical model is a normal or skewed normal one. For this to not
be true we need some really extreme distribution, like most of the time on one
end, but has some chance for an outlier value. However, in these cases by just
increasing the mean with a constant multiplier (like to $40$) may already result
in a $90\%$ confidence value. 

Nevertheless, we also need to consider if estimating the mean is a meaningful
thing to do. Remeber that the mean is not a robust measurement, because it's
effected by outliers, something that is true for the distribution too.

A method for constructing a confidence interval is robust if the resulting
confidence intervals include the theoretical paramter approximately the
percentage of time claimed by the confidence level, even if the necessary
condition for the confidence interval isn't satisfied.
